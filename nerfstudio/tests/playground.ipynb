{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_map = torch.zeros((3, 5, 8))\n",
    "weights_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 3, 5],\n",
      "        [1, 4, 7],\n",
      "        [2, 1, 6]])\n",
      "tensor([[ 0],\n",
      "        [29],\n",
      "        [79],\n",
      "        [94]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_map[2, 1, 6] = 1  # 94\n",
    "weights_map[0, 3, 5] = 1  # 29\n",
    "weights_map[1, 4, 7] = 1  # 79\n",
    "weights_map[0, 0, 0] = 1  # 94\n",
    "\n",
    "print(torch.nonzero(weights_map))\n",
    "print(torch.nonzero(weights_map.flatten()))\n",
    "\n",
    "height, width, depth = weights_map.shape\n",
    "\n",
    "# x * (W * D) + y * D + z\n",
    "# elem_(-3) * (dim_(-2) * dim(-1)) + elem_(-2) * dim(-1) + elem_(-1)\n",
    "\n",
    "0 * (width * depth) + 0 * depth + 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 3, 5],\n",
      "        [1, 4, 7],\n",
      "        [2, 1, 6]])\n"
     ]
    }
   ],
   "source": [
    "# Pick multinomial in whole tensor\n",
    "# flatten = view(-1)\n",
    "K = torch.multinomial(weights_map.flatten(), 4, replacement=False)\n",
    "\n",
    "# Write a function that converts 1D indices to 3D indices\n",
    "def get_indices_3d(indices, shape):\n",
    "    indices_3d = torch.zeros((len(indices), len(shape)), dtype=torch.long)\n",
    "    for i in range(len(indices)):\n",
    "        for j in range(len(shape)):\n",
    "            indices_3d[i, j] = indices[i] % shape[j]\n",
    "            indices[i] = indices[i] // shape[j]\n",
    "    return indices_3d\n",
    "\n",
    "index_3d = (K // (weights_map.size(1) * weights_map.size(2)),\n",
    "            (K // weights_map.size(2)) % weights_map.size(1),\n",
    "            K % weights_map.size(2))\n",
    "\n",
    "print(torch.stack(index_3d, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 3, 5],\n",
      "        [1, 4, 7],\n",
      "        [2, 1, 6]])\n"
     ]
    }
   ],
   "source": [
    "num_samples = 4\n",
    "samples = torch.multinomial(weights_map.flatten(), num_samples, replacement=False)\n",
    "\n",
    "# Convert 1D indices to 3D indices\n",
    "# [num_images, H, W]\n",
    "batch_idx = torch.stack((K // (weights_map.size(1) * weights_map.size(2)),\n",
    "                        (K // weights_map.size(2)) % weights_map.size(1),\n",
    "                        K % weights_map.size(2)), \n",
    "                        dim=1)\n",
    "\n",
    "print(batch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_map = torch.zeros((400, 540, 960))\n",
    "# Set 10% of values of weights_map to 1\n",
    "weights_map[torch.rand(weights_map.shape) < 0.1] = 1\n",
    "\n",
    "num_samples = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 97, 429, 225],\n",
      "        [ 97,  84,  93],\n",
      "        [ 98, 327, 832],\n",
      "        [ 98,  84, 861],\n",
      "        [ 98,  26, 680],\n",
      "        [ 98, 200, 496],\n",
      "        [ 99, 451, 874],\n",
      "        [ 99, 232, 299],\n",
      "        [ 99, 437, 599],\n",
      "        [ 99, 181, 446]])\n",
      "936 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "\n",
    "# about 1s for 400 samples over 1000 540p images.\n",
    "\n",
    "indices = torch.zeros((num_samples, 3), dtype=torch.long)\n",
    "sampled_pixels = 0\n",
    "\n",
    "pixels_per_iter = 4\n",
    "nb_iters = int(num_samples / pixels_per_iter)\n",
    "\n",
    "# Old sampling\n",
    "for i in range(nb_iters):\n",
    "    weights_map_i = weights_map[i]\n",
    "    samples = torch.multinomial(weights_map_i.flatten(), pixels_per_iter, replacement=False)\n",
    "    h, w = torch.div(samples, 960, rounding_mode=\"floor\"), samples % 960\n",
    "    indices[sampled_pixels : (sampled_pixels + pixels_per_iter), 0] = i\n",
    "    indices[sampled_pixels : (sampled_pixels + pixels_per_iter), 1] = h\n",
    "    indices[sampled_pixels : (sampled_pixels + pixels_per_iter), 2] = w\n",
    "    sampled_pixels += pixels_per_iter\n",
    "\n",
    "print(indices[-10:, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3])\n"
     ]
    }
   ],
   "source": [
    "#%%timeit -n 1 -r 1\n",
    "\n",
    "# New sampling\n",
    "# NB: Multinomial is for max 2^24 values = 16777216\n",
    "# 1000*960*540 = about 2^29 (> 28, < 29)\n",
    "num_images, height, width = weights_map.shape\n",
    "images_per_iter = 20\n",
    "\n",
    "indices = torch.zeros((num_samples, 3), dtype=torch.long)\n",
    "\n",
    "for i in range(0, num_images, images_per_iter):\n",
    "    weights_map_i = weights_map[i : i + images_per_iter]\n",
    "    samples = torch.multinomial(weights_map_i.flatten(), int(num_samples / images_per_iter), replacement=False)\n",
    "    indices = torch.stack((samples // (weights_map_i.size(1) * weights_map_i.size(2)),\n",
    "                            (samples // weights_map_i.size(2)) % weights_map_i.size(1),\n",
    "                            samples % weights_map_i.size(2)), \n",
    "                            dim=1)\n",
    "    indices[:, 0] += i\n",
    "\n",
    "# 3.6 seconds, not worth it.\n",
    "\n",
    "print(indices.shape)\n",
    "\n",
    "#print(indices[-10:, :])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
